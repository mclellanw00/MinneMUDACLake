{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c598dd16",
   "metadata": {},
   "source": [
    "# Project 2 Lab 6 - Parcel Feature Extraction\n",
    "\n",
    "Next, we will illustrate the construction of features related to our main task: finding the relationship between property development and water quality over time.  In a previous lab, you identified lakes for which we have complete information for the years from 2004 to 2015.  In this lab, we will\n",
    "\n",
    "[Original Data and variable information](https://gisdata.mn.gov/organization/us-mn-state-metrogis?q=Metro+Regional+Parcel+Dataset&sort=score+desc%2C+metadata_modified+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb044c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 08:44:54 WARN Utils: Your hostname, lu4543hm221 resolves to a loopback address: 127.0.1.1; using 192.168.26.203 instead (on interface eth0)\n",
      "22/12/08 08:44:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 08:44:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, pandas_udf\n",
    "from more_pyspark import to_pandas, recode\n",
    "spark = (SparkSession.builder.appName('Ops')\n",
    "         .getOrCreate())\n",
    "from composable.glob import glob\n",
    "from composable.strict import map, star_map, filter, sorted\n",
    "from composable.sequence import reduce\n",
    "from composable import pipeable\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from composable.tuple import split_by\n",
    "from composable import pipeable\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21453ff5",
   "metadata": {},
   "source": [
    "## Problem 1 - Read/filter/union/write the combined parcel data\n",
    "\n",
    "Next, we will use a pipe to read, filter, union and write the parcel data, with the resulting file being partitioned by the year and lake ID.\n",
    "\n",
    "#### Tasks 1\n",
    "\n",
    "Create a pipe that\n",
    "\n",
    "1. Finds a list of paths for the parquet \"files\" created in a previous lab.\n",
    "    * We only needs the years 2004-2014.\n",
    "2. Reads/filters each of the parcel parquet files by mapping the first helper function from the last step to each path. \n",
    "    * You should use the imported list of lakes with complete information to filter on lakes.\n",
    "    * Use the distance category to only include parcels within 1600 m of the respective lake. \n",
    "    * You can drop the centroid lat & long, only with the distance information, once the filters are applied.\n",
    "3. Union the resulting data frames into one data frame.\n",
    "\n",
    "#### Task 2.\n",
    "\n",
    "Write the combined parcel file to a parquet file that is partitioned by the lake ID and year (in that order).  This is our silver table for the parcel data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40439ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./parcel_2004.parquet',\n",
       " './parcel_2005.parquet',\n",
       " './parcel_2006.parquet',\n",
       " './parcel_2007.parquet',\n",
       " './parcel_2008.parquet',\n",
       " './parcel_2009.parquet',\n",
       " './parcel_2010.parquet',\n",
       " './parcel_2011.parquet',\n",
       " './parcel_2012.parquet',\n",
       " './parcel_2013.parquet',\n",
       " './parcel_2014.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels = sorted('./parcel_20*' >> glob)\n",
    "\n",
    "parcels_minus = parcels[:-1]\n",
    "parcels_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78d9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 22:54:29 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from lake import lake_complete\n",
    "\n",
    "read_parcels = lambda path: (spark.read.parquet(path, header=True, sep='|')\n",
    "                            .where((col('Monit_MAP_CODE1').isin(lake_complete))&(col('distance')!= 'Over 1600'))\n",
    "                            .drop('centroid_long','centroid_lat')\n",
    ")\n",
    "\n",
    "parcel_join = (parcels_minus\n",
    "            >>map(read_parcels)\n",
    "            >>reduce(lambda df, df2: df.union(df2).distinct())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f76645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lake_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb3ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 19:07:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                        (0 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[204.757s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 46.0 (TID 10229): Retried waiting for GCLocker too often allocating 262146 words\n",
      "22/12/06 19:08:28 WARN TaskMemoryManager: Failed to allocate a page (2097152 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# (parcel_join\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('allparcels.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afac4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "allparcels = spark.read.parquet('./allparcels.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74814206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1357337"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allparcels.count() #-- THIS COUNT TAKES TOO DAMN LONG TO RUN DO NOT RUN AGAIN JUST FOR FUTURE REF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f79a5",
   "metadata": {},
   "source": [
    "> many many rows after we union(ized) all the parcel files together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528567b",
   "metadata": {},
   "source": [
    "## Problem 2 - Feature construction\n",
    "\n",
    "**Overview.** Remember that our target output file will have one row per year-lake combination.  To attach property information, we will need to group and aggregate the parcel data to create features for each lake-year combination.  When grouping the data, be sure to maintain the variables needed to join to the water quality data, namely the lake ID and year.  Since we are looking at tracking property development/change over time, we will want to generate features tracking\n",
    "\n",
    "* Number of properties close to each lake,\n",
    "* The value of properties close to each lake,\n",
    "* Aggregate size and type of the properties, and\n",
    "* Other features that might impact water quality.\n",
    "    \n",
    "#### Task 1. Understanding parcel variables\n",
    "\n",
    "Before we can construct features, we need to make sure we understand the parcel data.  The metro parcel data is provided by the State of Minnesota and the meta data can be found online.  For example, searching for *metro parcel 2014* lead to [this site](https://geo.btaa.org/catalog/304cf3d8-a53b-4ea9-b02a-f550bd68e320).  Clicking on the *Meta data* button in the top left, brought up more information.  Clicking *Download* opened in this meta data [in a separate page](https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metrogis/plan_regonal_parcels_2014/metadata/metadata.html)\n",
    "\n",
    "Look through the **Section 4: Attributes** and identify variables that might impact the water quality of near-by lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fe305",
   "metadata": {},
   "source": [
    "> <font color=\"orange\"> From doing research on the watershed district attribute, it seems like that could impact the quality of water in the lakes as the goal is to fix/solve water related issues in the lakes - also agricultural preserve could have an impoact on the quality of water due to the overall quality of the land residing around it and see how that overall affect changes around lakes. the other attributes then under the agricultural preserve because if it is enrolled in the program then it is focused on keeping the land preserved and if it is expiring then it focuses on whether or not the lake is on the verge of being in poorer quality</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925c5dc",
   "metadata": {},
   "source": [
    "#### Task 2. Brainstorm about features\n",
    "\n",
    "Remember that we need to aggregate down to a table with one row per lake-year, which means that feature construction will involve computing summary statistics. Below are some techniques for feature construction that you might employ.\n",
    "\n",
    "1. **Numerical summaries.** For numeric variables, you could should compute one or more summary statistics (mean, median, SD, IQR, etc.) per group.\n",
    "2. **Categorical summaries.**. For text data, we will have some more work.  Here are some strategies.\n",
    "    * **Success rates.** Compute success rates for binary variables.  For example, we could compute the percent/fraction of residences that have a basement.\n",
    "    * **Clean labels.** Be sure to inspect the unique labels and clean up duplicate/similar labels.\n",
    "    * **Make broader classifications.**  Some categorical variables will have too many categories that apply to a small number of properties.  These should be recoded into a smaller set of broad categories.  Try to eliminate or combine rare categories in the process.\n",
    "    * **Indicator columns.** Another strategy is to create indicator variables then aggregate, where the result can be either zero-one (presence/absence) or the total/proportion over all rows.  For example, we could create the number of properties of each use type.\n",
    "\n",
    "Consider the variables you identified in the last step, and develop a feature construction strategy for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825bf8c6",
   "metadata": {},
   "source": [
    "> <font color=\"orange\"> The watershed district attribute is a text value attribute (WSHD_DIST) and grab then each of the unique different names associated with the lakes and DNR ID's - there can be calculated rates though for it though on what the levy is in that district while some can be - having no values which can be misleading whether there is a levy tax of 0 or there truly is none\n",
    ">> Agricultural Preserve is a binary column (AG_PRESERV) which can be created into indicators or filtered down to only lakes that are YES and then can create statstical summaries on lakes that are preserved vs the ones that are not\n",
    ">>>The other two agricultural preserves are date columns which can then do a year by year breakdown for each year - you could cast the cloumns to date format (assuming they are in YYYY-DD-MM format)\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545209f",
   "metadata": {},
   "source": [
    "#### Task 3.  Numerical Summaries\n",
    "\n",
    "Two important categories of property data involve the size (e.g., finished square footage) and value (e.g., accessed value and/or taxes paid).\n",
    "\n",
    "**Tasks.** \n",
    "\n",
    "1. Identify 2-3 variables for each of these categories.\n",
    "2. Write a query that computes the summary statistics for each of these variables for each lake-year.  \n",
    "3. Write this summary table out to a parquet file named `parcel_numerical_summaries.parquet`.  Again, you should partition by lake ID and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66c67b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONIT_MAP_CODE1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Max_Estimated_Total</th>\n",
       "      <th>Avg_Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2004</td>\n",
       "      <td>97858.0</td>\n",
       "      <td>3467.759058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2005</td>\n",
       "      <td>94200.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2006</td>\n",
       "      <td>93600.0</td>\n",
       "      <td>3435.787510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2007</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>3230.503464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2008</td>\n",
       "      <td>99900.0</td>\n",
       "      <td>3317.029727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>82036800-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>961400.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>82036800-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>985500.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>82036800-01</td>\n",
       "      <td>2012</td>\n",
       "      <td>954300.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>82036800-01</td>\n",
       "      <td>2013</td>\n",
       "      <td>963300.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>82036800-01</td>\n",
       "      <td>2014</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MONIT_MAP_CODE1  Year Max_Estimated_Total      Avg_Tax\n",
       "0       02000500-01  2004             97858.0  3467.759058\n",
       "1       02000500-01  2005             94200.0     0.000000\n",
       "2       02000500-01  2006             93600.0  3435.787510\n",
       "3       02000500-01  2007             99500.0  3230.503464\n",
       "4       02000500-01  2008             99900.0  3317.029727\n",
       "..              ...   ...                 ...          ...\n",
       "523     82036800-01  2010            961400.0     0.000000\n",
       "524     82036800-01  2011            985500.0     0.000000\n",
       "525     82036800-01  2012            954300.0     0.000000\n",
       "526     82036800-01  2013            963300.0     0.000000\n",
       "527     82036800-01  2014              9500.0     0.000000\n",
       "\n",
       "[528 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here.\n",
    "from pyspark.sql.functions import mean, stddev, max, percent_rank, sum, regexp_extract, regexp_replace, column\n",
    "\n",
    "numsum = (allparcels\n",
    "        .groupBy(col('MONIT_MAP_CODE1'), col('Year'))\n",
    "        .agg(max(col('EMV_TOTAL')).alias('Max_Estimated_Total'), \n",
    "             mean(col('TOTAL_TAX')).alias('Avg_Tax'))\n",
    ")\n",
    "numsum.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40cb155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#already ran this code chunk parquet file exists\n",
    "\n",
    "# (numsum\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('parcel_numerical_summaries.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82c0f1",
   "metadata": {},
   "source": [
    "## Problem 3.  Simple categorical summaries.\n",
    "\n",
    "In this part, you will create summary statistics for some of the simpler categorical variables.\n",
    "\n",
    "**Binary variables.** There are two examples of binary variables, listed below.  You will need to compute the percent of `Yes` for each.\n",
    "\n",
    "* GARAGE: Garage Y/N\n",
    "* BASEMENT: Basement Y/N\n",
    "\n",
    "**Other categorical variables.** There are a number of other categorical variables.  You need to select one of these variables, inspect/clean your variable as needed, create indicator variables for each resulting label, and compute summary statistics for each label.\n",
    "\n",
    "* HOMESTEAD: Homestead Status\n",
    "* TAX_EXEMPT: Tax Exempt Status \n",
    "* DWELL_TYPE: Dwelling Type \n",
    "* HOME_STYLE: Home Style\n",
    "* HEATING: Heating type\n",
    "* COOLING: Cooling type\n",
    "\n",
    "**Tasks.**\n",
    "Create a query that\n",
    "\n",
    "1. Select one binary and two other categorical variables for feature construction.\n",
    "2. Reads in the parcel data and selects the relevant columns (be sure to keep the lake ID and year).\n",
    "3. Inspect unique labels and recode/clean as needed.\n",
    "4. Creates indicator columns for all labels.\n",
    "5. Groups/aggregates to compute summary statistics for each lake year.\n",
    "\n",
    "Write this summary table out to a parquet file named `parcel_categorical_summaries.parquet`.  Again, you should partition by lake ID and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318e02c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=====================================================>(197 + 1) / 198]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|HEATING   |\n",
      "+----------+\n",
      "|FA Gas    |\n",
      "|null      |\n",
      "|Yes       |\n",
      "|0         |\n",
      "|Forced Air|\n",
      "|Electric  |\n",
      "|Gravity   |\n",
      "|Hot Water |\n",
      "|Other     |\n",
      "|H. Water  |\n",
      "|Oil F.A.  |\n",
      "|FORCED AIR|\n",
      "|No        |\n",
      "|FHA Gas   |\n",
      "|RAD/BBELEC|\n",
      "|SPACE HTR |\n",
      "|FRC AIR ND|\n",
      "|HOT WATER |\n",
      "|NONE      |\n",
      "|RAD INFRED|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bateman = allparcels.drop_duplicates(['HEATING'])\n",
    "trying = bateman.select(['HEATING']).distinct()\n",
    "trying.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb07f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, explode, struct, lit, col, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862f0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "basementrecode = {'Y':1, 'N':0, 'None':0}\n",
    "tax_exemptrec = {'Y':1, 'N':0, 'None':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4af3edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, 'Y', 'N']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(allparcels.select('TAX_EXEMPT').distinct().toPandas()['TAX_EXEMPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b7b0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, 'Y', 'N']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(allparcels.select('BASEMENT').distinct().toPandas()['BASEMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08191cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FA Gas',\n",
       " None,\n",
       " 'Yes',\n",
       " '0',\n",
       " 'Forced Air',\n",
       " 'Electric',\n",
       " 'Gravity',\n",
       " 'Hot Water',\n",
       " 'Other',\n",
       " 'H. Water',\n",
       " 'Oil F.A.',\n",
       " 'FORCED AIR',\n",
       " 'No',\n",
       " 'FHA Gas',\n",
       " 'RAD/BBELEC',\n",
       " 'SPACE HTR',\n",
       " 'FRC AIR ND',\n",
       " 'HOT WATER',\n",
       " 'NONE',\n",
       " 'RAD INFRED',\n",
       " 'GRAVITY/WA',\n",
       " 'STEAM W A/',\n",
       " 'LP',\n",
       " 'Wood',\n",
       " 'HOT AIR',\n",
       " 'AIR DUCTED',\n",
       " 'ENG F AIR',\n",
       " 'CONVECTION',\n",
       " 'IN FLOOR',\n",
       " 'FHA',\n",
       " 'ENG STEAM',\n",
       " 'ELEC BASBD',\n",
       " 'RAD WATER',\n",
       " 'ELECTRIC',\n",
       " 'SPACE HEAT',\n",
       " 'OTHER W A/',\n",
       " 'Evaporative Cooling',\n",
       " 'Forced Air Furnace',\n",
       " 'Electric Baseboard',\n",
       " 'Complete HVAC',\n",
       " 'Space Heater',\n",
       " 'Package Unit',\n",
       " 'Baseboard, Hot Water',\n",
       " 'Y',\n",
       " 'N',\n",
       " 'Radiant Space Heaters',\n",
       " 'Gravity Furnace',\n",
       " 'ELEC WALL',\n",
       " 'GEO THERM',\n",
       " 'RAD ELEC',\n",
       " 'Solar',\n",
       " 'STEAM',\n",
       " 'HEAT PUMP',\n",
       " 'SP HT W/FN',\n",
       " 'SPACE-FAN']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvac = list(allparcels.select('HEATING').distinct().toPandas()['HEATING'])\n",
    "hvac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac16ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basementrecode = {'Y':1, 'N':0, 'None':0}\n",
    "tax_exemptrec = {'Y':1, 'N':0, 'None':0}\n",
    "heatrec = {'FA Gas':1, None:0, 'Yes':1, '0':0, 'Forced Air':0,'Electric':1,'Gravity':0,'Hot Water':1,'Other':1,'H. Water':1,'Oil F.A.':1,'FORCED AIR':1,'No':0,'FHA Gas':0,\n",
    "'RAD/BBELEC':1,'SPACE HTR':1,'FRC AIR ND':1,'HOT WATER':1,'NONE':0,'RAD INFRED':1,'GRAVITY/WA':1,'STEAM W A/':1,'LP':1,'Wood':1,'HOT AIR':1,'AIR DUCTED':1,'ENG F AIR':1,\n",
    " 'CONVECTION':1,'IN FLOOR':1,'FHA':1,'ENG STEAM':1,'ELEC BASBD':1,'RAD WATER':1,'ELECTRIC':1,'SPACE HEAT':1,'OTHER W A/':1,'Evaporative Cooling':0,'Forced Air Furnace':1,\n",
    " 'Electric Baseboard':1,'Complete HVAC':1,'Space Heater':1,'Package Unit':1,'Baseboard, Hot Water':1,'Y':1,'N':0,'Radiant Space Heaters':1,'Gravity Furnace':1,'ELEC WALL':1,\n",
    " 'GEO THERM':1,'RAD ELEC':1,'Solar':1,'STEAM':1,'HEAT PUMP':1,'SP HT W/FN':1,'SPACE-FAN':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dff5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/column.py:419: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>MONIT_MAP_CODE1</th>\n",
       "      <th>Percent_Basement</th>\n",
       "      <th>Percent_Tax_Exempt</th>\n",
       "      <th>Percent_Heated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>82009400-01</td>\n",
       "      <td>0.945263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>82009400-01</td>\n",
       "      <td>0.945972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>82009400-01</td>\n",
       "      <td>0.945972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>27005300-01</td>\n",
       "      <td>0.495488</td>\n",
       "      <td>0.064807</td>\n",
       "      <td>0.033908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year MONIT_MAP_CODE1  Percent_Basement  Percent_Tax_Exempt  Percent_Heated\n",
       "0  2014     82009400-01          0.945263            0.000000        0.945263\n",
       "1  2012     82009400-01          0.945972            0.000000        0.945972\n",
       "2  2013     82009400-01          0.945972            0.000000        0.945972\n",
       "3  2013     27005300-01          0.495488            0.064807        0.033908"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "catsum = (allparcels\n",
    "        .select('BASEMENT', 'MONIT_MAP_CODE1', 'Year', 'TAX_EXEMPT', 'HEATING')\n",
    "        .withColumn('Basement_Ind', recode('BASEMENT', basementrecode, default=0))\n",
    "        .withColumn('Tax_Ind', recode('TAX_EXEMPT', tax_exemptrec, default=0))\n",
    "        .withColumn('Heat_Ind', recode('HEATING', heatrec, default=0))\n",
    "        .groupBy('Year', 'MONIT_MAP_CODE1')\n",
    "        .agg(mean(col('Basement_Ind')).alias('Percent_Basement'),(mean(col('Tax_Ind')).alias('Percent_Tax_Exempt')),(mean(col('Heat_Ind')).alias('Percent_Heated')))\n",
    ")\n",
    "catsum.take(4) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8272e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=================================================>    (182 + 8) / 198]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+--------------------+--------------------+--------------------+\n",
      "|Year|MONIT_MAP_CODE1|Percent_Basement    |Percent_Tax_Exempt  |Percent_Heated      |\n",
      "+----+---------------+--------------------+--------------------+--------------------+\n",
      "|2014|82009400-01    |0.9452626411389298  |0.0                 |0.9452626411389298  |\n",
      "|2012|82009400-01    |0.9459724950884086  |0.0                 |0.9459724950884086  |\n",
      "|2013|82009400-01    |0.9459724950884086  |0.0                 |0.9459724950884086  |\n",
      "|2013|27005300-01    |0.49548810500410173 |0.06480721903199343 |0.03390757451462948 |\n",
      "|2010|27005300-01    |0.008487337440109514|0.06351813826146475 |0.033949349760438056|\n",
      "|2014|27005300-01    |0.49562841530054647 |0.06448087431693988 |0.033879781420765025|\n",
      "|2010|82009400-01    |0.9467030242935052  |0.041398116013882005|0.9467030242935052  |\n",
      "|2014|27062700-01    |0.8828422388270125  |0.02354433948963767 |0.04370506132806993 |\n",
      "|2009|27005300-01    |0.00849780701754386 |0.06359649122807018 |0.034402412280701754|\n",
      "|2012|27005300-01    |0.49529395716818986 |0.0642477151821034  |0.0338289455735916  |\n",
      "|2008|27005300-01    |0.0                 |0.06368821292775666 |0.0                 |\n",
      "|2012|27004201-01    |0.6009213936078318  |0.034408292542470485|0.03714367981572128 |\n",
      "|2011|82009400-01    |0.945832302745486   |0.045263418253771955|0.945832302745486   |\n",
      "|2011|27005300-01    |0.008480372042128299|0.06373957051019012 |0.033921488168513196|\n",
      "|2009|27062700-01    |0.1591549295774648  |0.022676056338028168|0.04943661971830986 |\n",
      "|2011|27004201-01    |0.036574746008708275|0.036429608127721336|0.036574746008708275|\n",
      "|2013|27004201-01    |0.6027436823104693  |0.03581227436823105 |0.03725631768953069 |\n",
      "|2011|27062700-01    |0.15906531531531531 |0.022381756756756757|0.049408783783783786|\n",
      "|2012|27062700-01    |0.881167183535382   |0.022695235410205808|0.048632647307583876|\n",
      "|2013|27062700-01    |0.8811504300014098  |0.02326237135203722 |0.0484985196672776  |\n",
      "+----+---------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yes = catsum\n",
    "yes.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3af9246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# (catsum\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('parccel_categorical_variables.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e88d0",
   "metadata": {},
   "source": [
    "## Problem 4.  Join all the summaries.\n",
    "\n",
    "Finally, you need to join all the summaries created above, along with the water quality summaries created in a previous lab, into one overall summary file.  Write the resulting table to a CSV file named `water_quality_and_parcel_summaries_2004_to_2015.csv`.\n",
    "\n",
    "Next, we need to recode the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad53138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAKE_NAME</th>\n",
       "      <th>DNR_ID_Site_Number</th>\n",
       "      <th>AvgSecchi</th>\n",
       "      <th>AvgPhos</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Comfort Lake</td>\n",
       "      <td>13005300-01</td>\n",
       "      <td>1.954167</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colby Lake</td>\n",
       "      <td>82009400-01</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeMontreville Lake</td>\n",
       "      <td>82010100-01</td>\n",
       "      <td>3.038462</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forest Lake</td>\n",
       "      <td>82015900-01</td>\n",
       "      <td>1.957143</td>\n",
       "      <td>0.023929</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goggins Lake</td>\n",
       "      <td>82007700-01</td>\n",
       "      <td>1.088571</td>\n",
       "      <td>0.093286</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LAKE_NAME DNR_ID_Site_Number  AvgSecchi   AvgPhos  year\n",
       "0    Big Comfort Lake        13005300-01   1.954167  0.032250  2009\n",
       "1          Colby Lake        82009400-01   0.533333  0.125333  2009\n",
       "2  DeMontreville Lake        82010100-01   3.038462  0.022000  2009\n",
       "3         Forest Lake        82015900-01   1.957143  0.023929  2009\n",
       "4        Goggins Lake        82007700-01   1.088571  0.093286  2009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality = spark.read.parquet('./water_quality_by_year.parquet', header=True, sep='|')\n",
    "quality.take(5)>>to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a42c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONIT_MAP_CODE1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Percent_Basement</th>\n",
       "      <th>Percent_Tax_Exempt</th>\n",
       "      <th>Percent_Heated</th>\n",
       "      <th>Max_Estimated_Total</th>\n",
       "      <th>Avg_Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97858.0</td>\n",
       "      <td>3467.759058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.087489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94200.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.700730</td>\n",
       "      <td>0.084347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93600.0</td>\n",
       "      <td>3435.787510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.724403</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>3230.503464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.735756</td>\n",
       "      <td>0.079273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99900.0</td>\n",
       "      <td>3317.029727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MONIT_MAP_CODE1  Year  Percent_Basement  Percent_Tax_Exempt  Percent_Heated  \\\n",
       "0     02000500-01  2004          0.000000            0.085145             0.0   \n",
       "1     02000500-01  2005          0.750656            0.087489             0.0   \n",
       "2     02000500-01  2006          0.700730            0.084347             0.0   \n",
       "3     02000500-01  2007          0.724403            0.083141             0.0   \n",
       "4     02000500-01  2008          0.735756            0.079273             0.0   \n",
       "\n",
       "  Max_Estimated_Total      Avg_Tax  \n",
       "0             97858.0  3467.759058  \n",
       "1             94200.0     0.000000  \n",
       "2             93600.0  3435.787510  \n",
       "3             99500.0  3230.503464  \n",
       "4             99900.0  3317.029727  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catsum_with_binary = (catsum.join(numsum, on=['MONIT_MAP_CODE1', 'Year'], how='inner'))\n",
    "\n",
    "catsum_with_binary.take(5) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11a9a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "stupidyear = (catsum_with_binary\n",
    "    .withColumn(\"OfficialYear\", col('Year'))\n",
    "    .drop('Year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93591e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONIT_MAP_CODE1</th>\n",
       "      <th>Percent_Basement</th>\n",
       "      <th>Percent_Tax_Exempt</th>\n",
       "      <th>Percent_Heated</th>\n",
       "      <th>Max_Estimated_Total</th>\n",
       "      <th>Avg_Tax</th>\n",
       "      <th>OfficialYear</th>\n",
       "      <th>LAKE_NAME</th>\n",
       "      <th>DNR_ID_Site_Number</th>\n",
       "      <th>AvgSecchi</th>\n",
       "      <th>AvgPhos</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97858.0</td>\n",
       "      <td>3467.759058</td>\n",
       "      <td>2004</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.087489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.681667</td>\n",
       "      <td>0.210083</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.700730</td>\n",
       "      <td>0.084347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93600.0</td>\n",
       "      <td>3435.787510</td>\n",
       "      <td>2006</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.724403</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>3230.503464</td>\n",
       "      <td>2007</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>0.203714</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.735756</td>\n",
       "      <td>0.079273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99900.0</td>\n",
       "      <td>3317.029727</td>\n",
       "      <td>2008</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.148833</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.745665</td>\n",
       "      <td>0.079273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98600.0</td>\n",
       "      <td>3489.229562</td>\n",
       "      <td>2009</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.758877</td>\n",
       "      <td>0.080925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98200.0</td>\n",
       "      <td>3384.645747</td>\n",
       "      <td>2010</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.768404</td>\n",
       "      <td>0.082713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99900.0</td>\n",
       "      <td>3459.313482</td>\n",
       "      <td>2011</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.119417</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.701740</td>\n",
       "      <td>0.081193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97900.0</td>\n",
       "      <td>3460.064623</td>\n",
       "      <td>2012</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.699918</td>\n",
       "      <td>0.079143</td>\n",
       "      <td>0.825227</td>\n",
       "      <td>981400.0</td>\n",
       "      <td>3319.081616</td>\n",
       "      <td>2013</td>\n",
       "      <td>George Watch Lake</td>\n",
       "      <td>02000500-01</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MONIT_MAP_CODE1  Percent_Basement  Percent_Tax_Exempt  Percent_Heated  \\\n",
       "0     02000500-01          0.000000            0.085145        0.000000   \n",
       "1     02000500-01          0.750656            0.087489        0.000000   \n",
       "2     02000500-01          0.700730            0.084347        0.000000   \n",
       "3     02000500-01          0.724403            0.083141        0.000000   \n",
       "4     02000500-01          0.735756            0.079273        0.000000   \n",
       "5     02000500-01          0.745665            0.079273        0.000000   \n",
       "6     02000500-01          0.758877            0.080925        0.000000   \n",
       "7     02000500-01          0.768404            0.082713        0.000000   \n",
       "8     02000500-01          0.701740            0.081193        0.000000   \n",
       "9     02000500-01          0.699918            0.079143        0.825227   \n",
       "\n",
       "  Max_Estimated_Total      Avg_Tax  OfficialYear          LAKE_NAME  \\\n",
       "0             97858.0  3467.759058          2004  George Watch Lake   \n",
       "1             94200.0     0.000000          2005  George Watch Lake   \n",
       "2             93600.0  3435.787510          2006  George Watch Lake   \n",
       "3             99500.0  3230.503464          2007  George Watch Lake   \n",
       "4             99900.0  3317.029727          2008  George Watch Lake   \n",
       "5             98600.0  3489.229562          2009  George Watch Lake   \n",
       "6             98200.0  3384.645747          2010  George Watch Lake   \n",
       "7             99900.0  3459.313482          2011  George Watch Lake   \n",
       "8             97900.0  3460.064623          2012  George Watch Lake   \n",
       "9            981400.0  3319.081616          2013  George Watch Lake   \n",
       "\n",
       "  DNR_ID_Site_Number  AvgSecchi   AvgPhos  year  \n",
       "0        02000500-01   0.705000  0.199000  2004  \n",
       "1        02000500-01   0.681667  0.210083  2005  \n",
       "2        02000500-01   0.728571  0.164286  2006  \n",
       "3        02000500-01   0.562857  0.203714  2007  \n",
       "4        02000500-01   0.550000  0.148833  2008  \n",
       "5        02000500-01   0.538000  0.105600  2009  \n",
       "6        02000500-01   0.493333  0.173000  2010  \n",
       "7        02000500-01   0.973333  0.119417  2011  \n",
       "8        02000500-01   0.359000  0.264900  2012  \n",
       "9        02000500-01   0.365000  0.310500  2013  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarywater = (stupidyear.join(quality, on=[(stupidyear.MONIT_MAP_CODE1 == quality.DNR_ID_Site_Number),\n",
    "                                             (stupidyear.OfficialYear == quality.year)], how='left'))\n",
    "summarywater.take(10)>>to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "894000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_quality_and_parcel_summaries = (summarywater\n",
    "            .drop(column('year'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "122a7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_quality_and_parcel_summaries.write.csv('./water_quality_and_parcel_summaries_2004_2014.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bf2b43c",
   "metadata": {},
   "source": [
    "^^ I already wrote that file so line is commented out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03c8ef",
   "metadata": {},
   "source": [
    "## Problem 5.  Put it all together\n",
    "\n",
    "It is often useful to package all of the data constructions steps together in one convenient place.  Your last task is to\n",
    "\n",
    "1. Gather all of your data construction code below.\n",
    "    * You don't need to include exploratory code, e.g., exploring join mismatches; only the code necessary to combine, clean, and write your data.\n",
    "2. Clean/refactor the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5b335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, pandas_udf, regexp_extract, year, to_date, avg\n",
    "from more_pyspark import to_pandas, recode\n",
    "spark = (SparkSession.builder.appName('Ops')\n",
    "         .getOrCreate())\n",
    "from composable.strict import map, star_map, filter, sorted\n",
    "from composable.sequence import reduce\n",
    "from composable import pipeable\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from composable.tuple import split_by\n",
    "from pyspark.sql.types import IntegerType\n",
    "from glob import glob as orignal_glob\n",
    "from composable.glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bd5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/column.py:419: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m colsaggregated \u001b[39m=\u001b[39m (mces\u001b[39m.\u001b[39mfilter(col(\u001b[39m'\u001b[39m\u001b[39mTotal_Phosphorus_QUALIFIER\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m         \u001b[39m.\u001b[39mfilter(col(\u001b[39m'\u001b[39m\u001b[39mSecchi_Depth_QUALIFIER\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m         \u001b[39m.\u001b[39mfilter(col(\u001b[39m'\u001b[39m\u001b[39mSecchi_Depth_RESULT\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39misNotNull())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         \u001b[39m.\u001b[39mgroupBy(col(\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m), col(\u001b[39m'\u001b[39m\u001b[39mLAKE_NAME\u001b[39m\u001b[39m'\u001b[39m), col(\u001b[39m'\u001b[39m\u001b[39mDNR_ID_Site_Number\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         \u001b[39m.\u001b[39magg(avg(col(\u001b[39m'\u001b[39m\u001b[39mSecchi_Depth_RESULT\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39malias(\u001b[39m'\u001b[39m\u001b[39mAvgSecchi\u001b[39m\u001b[39m'\u001b[39m), avg(col(\u001b[39m'\u001b[39m\u001b[39mTotal_Phosphorus_RESULT\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39malias(\u001b[39m'\u001b[39m\u001b[39mAvgPhos\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m water_quality_by_year \u001b[39m=\u001b[39m (colsaggregated)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m watermadetolist\u001b[39m=\u001b[39m (water_quality_by_year\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mDNR_ID_Site_Number\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m             \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39maddingstuff\u001b[39m\u001b[39m'\u001b[39m, recode(\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m, newfunc, default\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m             \u001b[39m.\u001b[39mgroupBy(\u001b[39m'\u001b[39m\u001b[39mDNR_ID_Site_Number\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m             \u001b[39m.\u001b[39magg(\u001b[39msum\u001b[39;49m(col(\u001b[39m'\u001b[39;49m\u001b[39maddingstuff\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39malias(\u001b[39m'\u001b[39m\u001b[39maddingstuff\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m             \u001b[39m.\u001b[39mfilter(col(\u001b[39m'\u001b[39m\u001b[39maddingstuff\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m==\u001b[39m\u001b[39m11\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m             \u001b[39m.\u001b[39mtoPandas()[\u001b[39m'\u001b[39m\u001b[39mDNR_ID_Site_Number\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m listedwater \u001b[39m=\u001b[39m (watermadetolist)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/wil/github-classroom/wsu-stat489/project-2-minnemudac-lakes-and-properties-mclellanw00/Project_2_Part_6_parcel_feature_extraction.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m sortedwater \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39msorted\u001b[39m(listedwater))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/column.py:560\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumn is not iterable\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 09:03:15 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 157369 ms exceeds timeout 120000 ms\n",
      "22/12/08 09:03:15 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "from parcel import sort_common_cols_2004_to_2015\n",
    "from lake import lake_complete\n",
    "\n",
    "glob = pipeable(orignal_glob)\n",
    "parcel1_files = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/201*parcel*.txt'))\n",
    "parcel2_files = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/200[456789]*parcel*.txt'))\n",
    "parcel_files=(parcel2_files + parcel1_files)\n",
    "\n",
    "\n",
    "getcol = (parcel_files\n",
    "                >> map(lambda path: spark.read.csv(path, header=True, sep='|'))\n",
    "                >> map(lambda df: df.columns)\n",
    "                >> map(lambda columnar: set(columnar))\n",
    ")\n",
    "\n",
    "update_set = lambda final, cols: final.intersection(cols)\n",
    "\n",
    "reducingcols = (getcol\n",
    "            >>reduce(update_set)\n",
    ")\n",
    "sort_common_cols_2004_to_2015 = (sorted(list(reducingcols)))\n",
    "\n",
    "# with open ('parcel.py', 'w') as pywrite:\n",
    "#     pywrite.write(f'sort_common_cols_2004_to_2015 = {sort_common_cols_2004_to_2015}')\n",
    "\n",
    "xref_file = spark.read.csv('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/Parcel_Lake_Monitoring_Site_Xref.txt',\n",
    "                    header=True,\n",
    "                    sep='\\t')\n",
    "\n",
    "water_quality = spark.read.csv('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/mces_lakes_1999_2014.txt',\n",
    "                              header = True,\n",
    "                              sep='\\t')\n",
    "\n",
    "xrefcolsel = (xref_file.select('Monit_MAP_CODE1', 'centroid_lat', 'centroid_long', 'Distance_Parcel_Monitoring_Site_meters', 'Distance_Parcel_Lake_meters'))\n",
    "\n",
    "xrefcolmutate= (xref_file.withColumn('distance', when(col('Distance_Parcel_Lake_meters') < 500, 'Less Than 500')\n",
    "                                        .when(col('Distance_Parcel_Lake_meters') > 1600, \"Over 1600\")\n",
    "                                        .when(col('Distance_Parcel_Lake_meters') <= 1600, \"Between 500 and 1600\")\n",
    "                                        .otherwise('Unknown Distance')))\n",
    "\n",
    "\n",
    "xref_selected = (xrefcolsel, xrefcolmutate)\n",
    "\n",
    "# (xref_selected\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('xref_selected.parquet')\n",
    "# )\n",
    "\n",
    "# (water_quality\n",
    "#  .write\n",
    "#  .partitionBy('DNR_ID_Site_Number')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('water_quality.parquet')\n",
    "# )\n",
    "\n",
    "waterparq = spark.read.parquet('water_quality.parquet')\n",
    "xref_parq = spark.read.parquet('xref_selected.parquet/')\n",
    "\n",
    "read_parcel = lambda path: spark.read.csv(path, header=True, sep='|').select(sort_common_cols_2004_to_2015).join(xref_parq, on=['centroid_lat', 'centroid_long'], how='inner')\n",
    "\n",
    "compile_yr = re.compile('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/(\\d{4})_metro_tax_parcels.txt')\n",
    "get_year = lambda path: compile_yr.search(path).group(1)\n",
    "\n",
    "joined_parcels = (parcel_files\n",
    "                    >>map(split_by((read_parcel, get_year)))\n",
    ")\n",
    "\n",
    "to_parquet = lambda df, year:df.write.partitionBy('Monit_MAP_CODE1', 'distance').mode('overwrite').parquet(f'parcel_{year}.parquet')\n",
    "\n",
    "# (joined_parcels\n",
    "#     >>star_map(to_parquet)\n",
    "#     )\n",
    "\n",
    "newfunc = {'':1}\n",
    "\n",
    "mces = spark.read.csv('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/mces_lakes_1999_2014.txt', header=True, sep='\\t')\n",
    "\n",
    "colsaggregated = (mces.filter(col('Total_Phosphorus_QUALIFIER') == 'Approved')\n",
    "        .filter(col('Secchi_Depth_QUALIFIER') == 'Approved')\n",
    "        .filter(col('Secchi_Depth_RESULT').isNotNull())\n",
    "        .filter(col('Total_Phosphorus_RESULT').isNotNull())\n",
    "        .withColumn('year', regexp_extract(to_date(col('END_DATE')), r'\\d{4}', 0))\n",
    "        .filter((col('year') > 2003) & (col('year') <= 2014))\n",
    "        .groupBy(col('year'), col('LAKE_NAME'), col('DNR_ID_Site_Number'))\n",
    "        .agg(avg(col('Secchi_Depth_RESULT')).alias('AvgSecchi'), avg(col('Total_Phosphorus_RESULT')).alias('AvgPhos')))\n",
    "\n",
    "water_quality_by_year = (colsaggregated)\n",
    "\n",
    "watermadetolist= (water_quality_by_year.select('DNR_ID_Site_Number', 'year')\n",
    "            .withColumn('addingstuff', recode('year', newfunc, default=1))\n",
    "            .groupBy('DNR_ID_Site_Number')\n",
    "            .agg(sum(col('addingstuff')).alias('addingstuff'))\n",
    "            .filter(col('addingstuff')==11)\n",
    "            .toPandas()['DNR_ID_Site_Number'])\n",
    "\n",
    "listedwater = (watermadetolist)\n",
    "sortedwater = list(sorted(listedwater))\n",
    "\n",
    "# with open ('lake.py', 'w') as pywrite:\n",
    "#     pywrite.write(f'lake_complete = {(sortedwater)}')\n",
    "\n",
    "confirmation = (water_quality_by_year\n",
    "        .where(col('DNR_ID_Site_Number').isin(lake_complete) == True)        \n",
    ")\n",
    "\n",
    "# (confirmation\n",
    "#  .write\n",
    "#  .partitionBy('year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('water_quality_by_year.parquet')\n",
    "\n",
    "\n",
    "parcels = sorted('./parcel_20*' >> glob)\n",
    "parcels_minus = parcels[:-1]\n",
    "\n",
    "read_parcels = lambda path: (spark.read.parquet(path, header=True, sep='|')\n",
    "                            .where((col('Monit_MAP_CODE1').isin(lake_complete))&(col('distance')!= 'Over 1600'))\n",
    "                            .drop('centroid_long','centroid_lat')\n",
    ")\n",
    "\n",
    "parcel_join = (parcels_minus\n",
    "            >>map(read_parcels)\n",
    "            >>reduce(lambda df, df2: df.union(df2).distinct())\n",
    ")\n",
    "\n",
    "# (parcel_join\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('allparcels.parquet')\n",
    "# )\n",
    "\n",
    "allparcels = spark.read.parquet('./allparcels.parquet/')\n",
    "\n",
    "binaryaggregates = (allparcels\n",
    "        .groupBy(col('MONIT_MAP_CODE1'), col('Year'))\n",
    "        .agg(max(col('EMV_TOTAL')).alias('Max_Estimated_Total'), \n",
    "             mean(col('TOTAL_TAX')).alias('Avg_Tax')))\n",
    "\n",
    "numsum = (binaryaggregates)\n",
    "\n",
    "# (numsum\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('parcel_numerical_summaries.parquet')\n",
    "# )\n",
    "\n",
    "categoryaggregates = (allparcels.select('BASEMENT', 'MONIT_MAP_CODE1', 'Year', 'TAX_EXEMPT', 'HEATING')\n",
    "        .withColumn('Basement_Ind', recode('BASEMENT', basementrecode, default=0))\n",
    "        .withColumn('Tax_Ind', recode('TAX_EXEMPT', tax_exemptrec, default=0))\n",
    "        .withColumn('Heat_Ind', recode('HEATING', heatrec, default=0))\n",
    "        .groupBy('Year', 'MONIT_MAP_CODE1')\n",
    "        .agg(mean(col('Basement_Ind')).alias('Percent_Basement'),(mean(col('Tax_Ind')).alias('Percent_Tax_Exempt')),(mean(col('Heat_Ind')).alias('Percent_Heated'))))\n",
    "\n",
    "basementrecode = {'Y':1, 'N':0, 'None':0}\n",
    "tax_exemptrec = {'Y':1, 'N':0, 'None':0}\n",
    "heatrec = {'FA Gas':1, None:0, 'Yes':1, '0':0, 'Forced Air':0,'Electric':1,'Gravity':0,'Hot Water':1,'Other':1,'H. Water':1,'Oil F.A.':1,'FORCED AIR':1,'No':0,'FHA Gas':0,\n",
    "'RAD/BBELEC':1,'SPACE HTR':1,'FRC AIR ND':1,'HOT WATER':1,'NONE':0,'RAD INFRED':1,'GRAVITY/WA':1,'STEAM W A/':1,'LP':1,'Wood':1,'HOT AIR':1,'AIR DUCTED':1,'ENG F AIR':1,\n",
    " 'CONVECTION':1,'IN FLOOR':1,'FHA':1,'ENG STEAM':1,'ELEC BASBD':1,'RAD WATER':1,'ELECTRIC':1,'SPACE HEAT':1,'OTHER W A/':1,'Evaporative Cooling':0,'Forced Air Furnace':1,\n",
    " 'Electric Baseboard':1,'Complete HVAC':1,'Space Heater':1,'Package Unit':1,'Baseboard, Hot Water':1,'Y':1,'N':0,'Radiant Space Heaters':1,'Gravity Furnace':1,'ELEC WALL':1,\n",
    " 'GEO THERM':1,'RAD ELEC':1,'Solar':1,'STEAM':1,'HEAT PUMP':1,'SP HT W/FN':1,'SPACE-FAN':1}\n",
    "\n",
    "catsum = (categoryaggregates)\n",
    "\n",
    "# (catsum\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1', 'Year')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('parccel_categorical_variables.parquet')\n",
    "# )\n",
    "\n",
    "quality = spark.read.parquet('./water_quality_by_year.parquet', header=True, sep='|')\n",
    "catsum_with_binary = (catsum.join(numsum, on=['MONIT_MAP_CODE1', 'Year'], how='inner'))\n",
    "\n",
    "stupidyear = (catsum_with_binary\n",
    "    .withColumn(\"OfficialYear\", col('Year'))\n",
    "    .drop('Year')\n",
    ")\n",
    "\n",
    "summarywater = (stupidyear.join(quality, on=[(stupidyear.MONIT_MAP_CODE1 == quality.DNR_ID_Site_Number),\n",
    "                                             (stupidyear.OfficialYear == quality.year)], how='left'))\n",
    "\n",
    "water_quality_and_parcel_summaries = (summarywater\n",
    "            .drop(column('year'))\n",
    ")\n",
    "\n",
    "# water_quality_and_parcel_summaries.write.csv('./water_quality_and_parcel_summaries_2004_2014.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b991b4",
   "metadata": {},
   "source": [
    "## Deliverables.\n",
    "\n",
    "Make sure you have pushed all of your lab notebooks, along with the final combined `CSV` to the GitHub Classroom repo.  Submit a WORD document on D2L that includes\n",
    "\n",
    "1. A link to your repository.\n",
    "2. Screen shots of verifying the construction of the larger parquet files. You don't (and probably can't) record all of the folders/files, but should be able to capture the basic structure/partitioning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('base': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "14ab39836ad45f9872a4fbaf347177c599900dde8b629b89551b87d8978983f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
