{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698db51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: more_pyspark in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: more-itertools<10.0.0,>=9.0.0 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from more_pyspark) (9.0.0)\n",
      "Requirement already satisfied: pandas<2,>=1 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from more_pyspark) (1.4.2)\n",
      "Requirement already satisfied: pyspark<4,>=3 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from more_pyspark) (3.3.1)\n",
      "Requirement already satisfied: composable>=0.4.0 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from more_pyspark) (0.5.3)\n",
      "Requirement already satisfied: toolz<0.12.0,>=0.11.1 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from composable>=0.4.0->more_pyspark) (0.11.2)\n",
      "Requirement already satisfied: python-forge<19.0,>=18.6 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from composable>=0.4.0->more_pyspark) (18.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from pandas<2,>=1->more_pyspark) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from pandas<2,>=1->more_pyspark) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from pandas<2,>=1->more_pyspark) (1.21.5)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from pyspark<4,>=3->more_pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas<2,>=1->more_pyspark) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install more_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23737b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: composable in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (0.5.3)\n",
      "Collecting composable\n",
      "  Downloading composable-0.5.4-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: toolz<0.12.0,>=0.11.1 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from composable) (0.11.2)\n",
      "Requirement already satisfied: python-forge<19.0,>=18.6 in /home/wil/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from composable) (18.6.0)\n",
      "Installing collected packages: composable\n",
      "  Attempting uninstall: composable\n",
      "    Found existing installation: composable 0.5.3\n",
      "    Uninstalling composable-0.5.3:\n",
      "      Successfully uninstalled composable-0.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "more-dfply 0.2.10 requires composable<0.3.0,>=0.2.5, but you have composable 0.5.4 which is incompatible.\u001b[0m\n",
      "Successfully installed composable-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install composable --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3054c",
   "metadata": {},
   "source": [
    "# Lab 4 - Creating partitioned parquet files\n",
    "\n",
    "In this lab, we will perform our first round of data preparation by writing the larger files (`XREF` and the yearly `parcel` files to the `parquet` format.  \n",
    "\n",
    "In the process, we will discuss and investigate an important concept in managing lots of data: the principle of locality.  Big data problem as IO bound, meaning that almost all of the time/resources will be used managing the input/output of data.  The principle of locality holds that is often reused over a short period of time (temporal locality) and data that is stored in similar locations tend to be used at similar points in a program (spatial locality).  The `parquet` always us to partition a data set to leverage these properties.   *The correct partitioning can result in orders of magnitude speed up in processing time!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ec68c",
   "metadata": {},
   "source": [
    "## The Principle of Locality.\n",
    "\n",
    "<img src=\"./img/locality.png\" width=\"800\">\n",
    "\n",
    "We can leverage the behavior of the operating system (OS)--in particular the loading of chucks of data in proximity and keeping that data in memory for a time--by partitioning our data so that similar data is stored together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b1b92",
   "metadata": {},
   "source": [
    "## We need to group the data by lake and distance to the lake\n",
    "\n",
    "<img src=\"./img/row_proximity.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d8102",
   "metadata": {},
   "source": [
    "## Problem 1 - Understanding the big picture and tables keys\n",
    "\n",
    "**Tasks.**  \n",
    "\n",
    "1. Explain why it, in the case of parcel data, to group the rows by lake id and distance to the lake.\n",
    "2. Neither of these columns is present in the parcel data files.  How will we go about adding this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca70000",
   "metadata": {},
   "source": [
    "> <font color=\"orange\"> we can take the lake id and distance from xref we need to group by lake id to get data on a lake by lake basis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7647b",
   "metadata": {},
   "source": [
    "## Problem 2 - Writing the XREF to a partitioned parquet \"file\"\n",
    "\n",
    "**Tasks.**\n",
    "\n",
    "1. Load the `XREF` data and select the relevant columns (Lake ID, centroid lat & long, distance to the lake).\n",
    "2. Create a new categorical variable named with three categories based on distance to the lake: withing 500m, between 501-1600m, and over 1600m.\n",
    "3. Write the table \n",
    "2. Read in each of these files and suggest the columns that will be used to join the tables.\n",
    "3. To understand the relationship (one-to-one; one-to-many; many-to-many) between tables, perform aggregation on each table to determine if there is one or many keys per row.\n",
    "4. Based on the results of the last task, suggest a join type and justify your response.\n",
    "5. For each table, create query that results in a column with one unique key per row.\n",
    "6. Perform the join suggested in **4.** and investigate any mismatches.  Document your findings and suggest necessary remedies.\n",
    "\n",
    "**Note.** The code for partitioning and writing a parquet file for the water quality data is provided as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b350e",
   "metadata": {},
   "source": [
    "#### Example - Writing a partitioned water quality file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc700300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:59:30 WARN Utils: Your hostname, lu4543hm221 resolves to a loopback address: 127.0.1.1; using 172.21.162.18 instead (on interface eth0)\n",
      "22/12/03 10:59:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 10:59:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from more_pyspark import to_pandas\n",
    "spark = (SparkSession.builder.appName('Ops')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da37af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parcel_PIN</th>\n",
       "      <th>Monit_MAP_CODE1</th>\n",
       "      <th>Monit_SITE_CODE</th>\n",
       "      <th>Monit_LAKE_SITE</th>\n",
       "      <th>Distance_Parcel_Monitoring_Site_meters</th>\n",
       "      <th>Lake_Hydroid</th>\n",
       "      <th>Distance_Parcel_Lake_meters</th>\n",
       "      <th>centroid_long</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>Parcel_pkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>19007900-01</td>\n",
       "      <td>19007900</td>\n",
       "      <td>1</td>\n",
       "      <td>2815.4927104148851</td>\n",
       "      <td>110517277058</td>\n",
       "      <td>2571.5267922258381</td>\n",
       "      <td>-93.11451</td>\n",
       "      <td>44.94283</td>\n",
       "      <td>2163034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>19007900-01</td>\n",
       "      <td>19007900</td>\n",
       "      <td>1</td>\n",
       "      <td>2753.4746875312162</td>\n",
       "      <td>110517277058</td>\n",
       "      <td>2515.3738022144425</td>\n",
       "      <td>-93.11539</td>\n",
       "      <td>44.94234</td>\n",
       "      <td>2163035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parcel_PIN Monit_MAP_CODE1 Monit_SITE_CODE Monit_LAKE_SITE  \\\n",
       "0       None     19007900-01        19007900               1   \n",
       "1       None     19007900-01        19007900               1   \n",
       "\n",
       "  Distance_Parcel_Monitoring_Site_meters  Lake_Hydroid  \\\n",
       "0                     2815.4927104148851  110517277058   \n",
       "1                     2753.4746875312162  110517277058   \n",
       "\n",
       "  Distance_Parcel_Lake_meters centroid_long centroid_lat Parcel_pkey  \n",
       "0          2571.5267922258381     -93.11451     44.94283     2163034  \n",
       "1          2515.3738022144425     -93.11539     44.94234     2163035  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xref_file = spark.read.csv('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/Parcel_Lake_Monitoring_Site_Xref.txt',\n",
    "                    header=True,\n",
    "                    sep='\\t'\n",
    ")\n",
    "\n",
    "xref_file.take(2)>>to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e86d61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monit_MAP_CODE1</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_long</th>\n",
       "      <th>Distance_Parcel_Monitoring_Site_meters</th>\n",
       "      <th>Distance_Parcel_Lake_meters</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98875</td>\n",
       "      <td>-93.4634</td>\n",
       "      <td>722.81806664277326</td>\n",
       "      <td>498.05795096568289</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98903</td>\n",
       "      <td>-93.46419</td>\n",
       "      <td>653.59523307741824</td>\n",
       "      <td>428.44441624563711</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98903</td>\n",
       "      <td>-93.46458</td>\n",
       "      <td>628.06603598632933</td>\n",
       "      <td>400.6929258452405</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98902</td>\n",
       "      <td>-93.46496</td>\n",
       "      <td>604.31870154847945</td>\n",
       "      <td>374.19147719578979</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98787</td>\n",
       "      <td>-93.46512</td>\n",
       "      <td>678.98349488074962</td>\n",
       "      <td>433.75850840810546</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.9876</td>\n",
       "      <td>-93.46511</td>\n",
       "      <td>701.3156946349593</td>\n",
       "      <td>454.60240145564489</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98729</td>\n",
       "      <td>-93.46509</td>\n",
       "      <td>728.07771881800522</td>\n",
       "      <td>480.19551012681632</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98787</td>\n",
       "      <td>-93.46434</td>\n",
       "      <td>723.26511107359954</td>\n",
       "      <td>481.83398431552473</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98815</td>\n",
       "      <td>-93.46435</td>\n",
       "      <td>702.12478158170904</td>\n",
       "      <td>463.45048693718456</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27010700-01</td>\n",
       "      <td>44.98842</td>\n",
       "      <td>-93.46436</td>\n",
       "      <td>682.44215863898751</td>\n",
       "      <td>447.03773991223932</td>\n",
       "      <td>Less Than 500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Monit_MAP_CODE1 centroid_lat centroid_long  \\\n",
       "0     27010700-01     44.98875      -93.4634   \n",
       "1     27010700-01     44.98903     -93.46419   \n",
       "2     27010700-01     44.98903     -93.46458   \n",
       "3     27010700-01     44.98902     -93.46496   \n",
       "4     27010700-01     44.98787     -93.46512   \n",
       "5     27010700-01      44.9876     -93.46511   \n",
       "6     27010700-01     44.98729     -93.46509   \n",
       "7     27010700-01     44.98787     -93.46434   \n",
       "8     27010700-01     44.98815     -93.46435   \n",
       "9     27010700-01     44.98842     -93.46436   \n",
       "\n",
       "  Distance_Parcel_Monitoring_Site_meters Distance_Parcel_Lake_meters  \\\n",
       "0                     722.81806664277326          498.05795096568289   \n",
       "1                     653.59523307741824          428.44441624563711   \n",
       "2                     628.06603598632933           400.6929258452405   \n",
       "3                     604.31870154847945          374.19147719578979   \n",
       "4                     678.98349488074962          433.75850840810546   \n",
       "5                      701.3156946349593          454.60240145564489   \n",
       "6                     728.07771881800522          480.19551012681632   \n",
       "7                     723.26511107359954          481.83398431552473   \n",
       "8                     702.12478158170904          463.45048693718456   \n",
       "9                     682.44215863898751          447.03773991223932   \n",
       "\n",
       "        distance  \n",
       "0  Less Than 500  \n",
       "1  Less Than 500  \n",
       "2  Less Than 500  \n",
       "3  Less Than 500  \n",
       "4  Less Than 500  \n",
       "5  Less Than 500  \n",
       "6  Less Than 500  \n",
       "7  Less Than 500  \n",
       "8  Less Than 500  \n",
       "9  Less Than 500  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xref_selected = (xref_file\n",
    "                .select('Monit_MAP_CODE1', 'centroid_lat', 'centroid_long', 'Distance_Parcel_Monitoring_Site_meters', 'Distance_Parcel_Lake_meters')\n",
    "                .withColumn('distance', when(col('Distance_Parcel_Lake_meters') < 500, 'Less Than 500')\n",
    "                                        .when(col('Distance_Parcel_Lake_meters') > 1600, \"Over 1600\")\n",
    "                                        .when(col('Distance_Parcel_Lake_meters') <= 1600, \"Between 500 and 1600\")\n",
    "                                        .otherwise('Unknown Distance'))\n",
    ")\n",
    "\n",
    "\n",
    "(xref_selected.where(col('distance') == 'Less Than 500')).take(10) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb0bc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "##FILE ALREADY RAN - NO NEED TO RUN AGAIN PARQUET EXISTS COMMENTED OUT\n",
    "\n",
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "# (xref_selected\n",
    "#  .write\n",
    "#  .partitionBy('Monit_MAP_CODE1')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('xref_selected.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aab109e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 11:52:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>DATA_SET_TITLE</th>\n",
       "      <th>LAKE_NAME</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>DNR_ID_Site_Number</th>\n",
       "      <th>MAJOR_WATERSHED</th>\n",
       "      <th>WATER_PLANNING_AUTHORITY</th>\n",
       "      <th>LAKE_SITE_NUMBER</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>Secchi_Depth_RESULT_SIGN</th>\n",
       "      <th>Secchi_Depth_RESULT</th>\n",
       "      <th>Secchi_Depth_QUALIFIER</th>\n",
       "      <th>Secchi_Depth_Units</th>\n",
       "      <th>Total_Phosphorus_RESULT_SIGN</th>\n",
       "      <th>Total_Phosphorus_RESULT</th>\n",
       "      <th>Total_Phosphorus_QUALIFIER</th>\n",
       "      <th>Total_Phosphorus_Units</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7108</td>\n",
       "      <td>Citizen Assisted Monitoring Program (CAMP) for...</td>\n",
       "      <td>Acorn Lake</td>\n",
       "      <td>Oakdale</td>\n",
       "      <td>Washington</td>\n",
       "      <td>82010200-01</td>\n",
       "      <td>Lower St. Croix River</td>\n",
       "      <td>Valley Branch WD</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>Approved</td>\n",
       "      <td>m</td>\n",
       "      <td>None</td>\n",
       "      <td>0.156</td>\n",
       "      <td>Approved</td>\n",
       "      <td>mg/L</td>\n",
       "      <td>-92.97171054</td>\n",
       "      <td>45.01655642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7108</td>\n",
       "      <td>Citizen Assisted Monitoring Program (CAMP) for...</td>\n",
       "      <td>Acorn Lake</td>\n",
       "      <td>Oakdale</td>\n",
       "      <td>Washington</td>\n",
       "      <td>82010200-01</td>\n",
       "      <td>Lower St. Croix River</td>\n",
       "      <td>Valley Branch WD</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-05-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>mg/L</td>\n",
       "      <td>-92.97171054</td>\n",
       "      <td>45.01655642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PROJECT_ID                                     DATA_SET_TITLE   LAKE_NAME  \\\n",
       "0       7108  Citizen Assisted Monitoring Program (CAMP) for...  Acorn Lake   \n",
       "1       7108  Citizen Assisted Monitoring Program (CAMP) for...  Acorn Lake   \n",
       "\n",
       "      CITY      COUNTY DNR_ID_Site_Number        MAJOR_WATERSHED  \\\n",
       "0  Oakdale  Washington        82010200-01  Lower St. Croix River   \n",
       "1  Oakdale  Washington        82010200-01  Lower St. Croix River   \n",
       "\n",
       "  WATER_PLANNING_AUTHORITY LAKE_SITE_NUMBER  START_DATE  ...  \\\n",
       "0         Valley Branch WD                1  2006-04-16  ...   \n",
       "1         Valley Branch WD                1  2006-05-01  ...   \n",
       "\n",
       "  Secchi_Depth_RESULT_SIGN Secchi_Depth_RESULT Secchi_Depth_QUALIFIER  \\\n",
       "0                     None                   1               Approved   \n",
       "1                     None                None                   None   \n",
       "\n",
       "  Secchi_Depth_Units Total_Phosphorus_RESULT_SIGN Total_Phosphorus_RESULT  \\\n",
       "0                  m                         None                   0.156   \n",
       "1                  m                         None                    None   \n",
       "\n",
       "  Total_Phosphorus_QUALIFIER Total_Phosphorus_Units     longitude     latitude  \n",
       "0                   Approved                   mg/L  -92.97171054  45.01655642  \n",
       "1                       None                   mg/L  -92.97171054  45.01655642  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_pyspark import to_pandas\n",
    "\n",
    "water_quality = spark.read.csv('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/mces_lakes_1999_2014.txt',\n",
    "                              header = True,\n",
    "                              sep='\\t')\n",
    "water_quality.take(2) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc3c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.74 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## SIMILAR TO ABOVE PARQUET ALREADY EXISTS SO COMMENTED OUT LINE \n",
    "\n",
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "# (water_quality\n",
    "#  .write\n",
    "#  .partitionBy('DNR_ID_Site_Number')\n",
    "#  .mode('overwrite')\n",
    "#  .parquet('water_quality.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f650f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "waterparq = spark.read.parquet('water_quality.parquet')\n",
    "xref_parq = spark.read.parquet('xref_selected.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a872f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2688766"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xref_parq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407da639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[centroid_lat: string, centroid_long: string, Distance_Parcel_Monitoring_Site_meters: string, Distance_Parcel_Lake_meters: string, distance: string, Monit_MAP_CODE1: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xref_parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a6ff385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[centroid_lat: string, centroid_long: string, Distance_Parcel_Monitoring_Site_meters: string, Distance_Parcel_Lake_meters: string, distance: string, Monit_MAP_CODE1: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xref_distinct = xref_parq.drop_duplicates(['Monit_MAP_CODE1'])\n",
    "xref_distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cc118",
   "metadata": {},
   "source": [
    "### Part 3 - Inspecting the partitioned `parquet` file\n",
    "\n",
    "**Tasks.** Inspect the resulting \"file\" (actually a folder) from the last set and answer the following questions.\n",
    "\n",
    "1. What impact did the partitioning have on the way the data was saved?\n",
    "2. How would this structure help `pyspark` apply predicate pushdown?\n",
    "3. How would this structure provide help via the principle of locality.  \n",
    "4. When working with a cluster of machines, operations such as `groupby` are WIDE operations, meaning they generally need to shuffle data between machines.  Such a suffle is *very* expensive.  In a future lab, we will be creating features for each labke by grouping and aggregating on the lakes and years.  How would applying a similar structure to the parcel data help in this case? **Hint.** Remember that the data will be distributed across multiple machines using the partitions, i.e., each machine will load all or some of the same partition(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3825a54",
   "metadata": {},
   "source": [
    "> <font color=\"orange\"> The data is being partionied similarly by xref monit map sites and dnr sites for water quality data - it contains multiple formats of data within them . I might be wrong with the pyspark predicate but being able to use the groupings specifically to combine and grab the water quality information on each site should be easy through pysparks dotchaining </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a402c6",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Key </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb449521",
   "metadata": {},
   "source": [
    "> <font color=\"orange\"> <b>1.</b> The \"file\" is actually a directory with sub-folders for each combo of labels for the partitioning variables.  <b>2.</b> <code>pyspark</code> can use the directory structure to totally combination that we filter out. <b>3.</b> Having the data partitioned/sorted should also help with the principle of locality by keeping similar data close and thus in memory at the same time.  When spreading our data across multiple machines, this will be particularly advantagous as each meaning can just load some/all of a partition, saving us having to spread data across multiple machines. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10464c",
   "metadata": {},
   "source": [
    "### Part 4 - Filter parcels and joining lake id\n",
    "\n",
    "Next, we will partition and write each of the 2004-2015 parcel files to a `parquet` \"file\".  To do this, complete each of the following tasks.\n",
    "\n",
    "**Tasks.**\n",
    "\n",
    "1. Write a helper functions that takes a parcel file path as input, reads corresponding CSV, selects the common columns (import from `parcel.py`), and joins on the necessary info from the `XREF` (lake ID, distance to the lake, distance category defined above, and centroid lat & long).\n",
    "2. Write a helper function that takes a `year` and parcel `df`, partitions the file by the lake ID and distance category, and writes the data to a \"file\" names `parcel_year.parquet`.\n",
    "3. Test the two helper functions on one of the parcel file years to make sure they are bug free.\n",
    "4. Write a pipe with a familiar shape\n",
    "    * Use `glob` to get all parcel file paths\n",
    "    * Filter the paths to 2004-2015\n",
    "    * split into year/df tuples using `get_year` and your helper function from **1.**.\n",
    "    * star_map your helper function from **2.** to write each of the files.\n",
    "    \n",
    "**Important note.** Each parcel files took 10+ minutes on my laptop, so running the whole pipe will take a while.  Pick a convenient time and be sure to plug in your laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6533b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from composable.glob import glob\n",
    "from composable.strict import map, star_map, filter, sorted\n",
    "from composable.sequence import reduce\n",
    "from composable import pipeable\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from composable.tuple import split_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f023f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your code/thoughts in one or more code/markdown cells, respectively.\n",
    "\n",
    "# parcel1 = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/200[456789]*parcel*.txt'))\n",
    "# parcel2 = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/201*parcel*.txt'))\n",
    "# parcel_files = (parcel1+parcel2)\n",
    "# from parcel import sort_common_cols_2004_to_2015\n",
    "# from pyspark.sql.functions import col\n",
    "# read_parcel = lambda path: spark.read.csv(path, header=True, sep='|').select(sort_common_cols_2004_t0_2015)\n",
    "# union_files = lambda out_df, df: out_df.distinct().union().distinct()\n",
    "\n",
    "# xref = (xref_distinct\n",
    "#     .select('centroid_long', 'centroid_lat', 'distance', 'MONIT_MAP_CODE1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49776336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2004_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2005_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2006_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2007_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2008_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2009_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2010_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2011_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2012_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2013_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2014_metro_tax_parcels.txt',\n",
       " './data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/2015_metro_tax_parcels.txt']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcel_files1 = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/200[456789]*parcel*.txt'))\n",
    "parcel_files2 = sorted(glob('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/201*parcel*.txt'))\n",
    "from parcel import sort_common_cols_2004_to_2015\n",
    "from pyspark.sql.functions import col\n",
    "parcel_files = (parcel_files1+parcel_files2)\n",
    "parcel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "793cd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parcel import sort_common_cols_2004_to_2015\n",
    "read_parcel = lambda path: spark.read.csv(path, header=True, sep='|').select(sort_common_cols_2004_to_2015).join(xref_parq, on=['centroid_lat', 'centroid_long'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e625d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "compile_yr = re.compile('./data/MinneMUDAC_raw_files/MinneMUDAC_raw_files/(\\d{4})_metro_tax_parcels.txt')\n",
    "get_year = lambda path: compile_yr.search(path).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2715aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[centroid_lat: string, centroid_long: string, ACRES_DEED: string, ACRES_POLY: string, AGPRE_ENRD: string, AGPRE_EXPD: string, AG_PRESERV: string, BASEMENT: string, BLDG_NUM: string, BLOCK: string, CITY: string, CITY_USPS: string, COOLING: string, COUNTY_ID: string, DWELL_TYPE: string, EMV_BLDG: string, EMV_LAND: string, EMV_TOTAL: string, FIN_SQ_FT: string, GARAGESQFT: string, GREEN_ACRE: string, HEATING: string, HOME_STYLE: string, LANDMARK: string, LOT: string, MULTI_USES: string, NUM_UNITS: string, OPEN_SPACE: string, OWNER_MORE: string, OWNER_NAME: string, OWN_ADD_L1: string, OWN_ADD_L2: string, OWN_ADD_L3: string, PARC_CODE: string, PIN: string, PLAT_NAME: string, PREFIXTYPE: string, PREFIX_DIR: string, SALE_DATE: string, SALE_VALUE: string, SCHOOL_DST: string, SPEC_ASSES: string, STREETNAME: string, STREETTYPE: string, SUFFIX_DIR: string, Shape_Area: string, Shape_Leng: string, TAX_ADD_L1: string, TAX_ADD_L2: string, TAX_ADD_L3: string, TAX_CAPAC: string, TAX_EXEMPT: string, TAX_NAME: string, TOTAL_TAX: string, UNIT_INFO: string, USE1_DESC: string, USE2_DESC: string, USE3_DESC: string, USE4_DESC: string, WSHD_DIST: string, XUSE1_DESC: string, XUSE2_DESC: string, XUSE3_DESC: string, XUSE4_DESC: string, YEAR_BUILT: string, Year: string, ZIP: string, ZIP4: string, Distance_Parcel_Monitoring_Site_meters: string, Distance_Parcel_Lake_meters: string, distance: string, Monit_MAP_CODE1: string],\n",
       " '2005')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_parcels = (parcel_files\n",
    "                    >>map(split_by((read_parcel, get_year)))\n",
    "                    )\n",
    "joined_parcels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "039cfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "to_parquet = lambda df, year:df.write.partitionBy('Monit_MAP_CODE1', 'distance').mode('overwrite').parquet(f'parcel_{year}.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c2097",
   "metadata": {},
   "source": [
    "NO NEED TO RUN CELL BELOW - ALREADY RAN AND THE PARCEL PARQUET FILES WERE CREATED IN 30 MINUTES - DO *NOT* RUN CELL BELOW WIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "193ff3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:>                                                        (0 + 8) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 15:54:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/12/01 15:54:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:>                                                        (0 + 8) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14704.831s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 96.0 (TID 1282): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "22/12/01 15:56:59 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "[14705.111s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 96.0 (TID 1282): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "22/12/01 15:57:00 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:>                                                       (0 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 16:05:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 16:06:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:==========================>                             (8 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 16:08:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:>                                                       (0 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 16:16:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:==========================>                             (8 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 16:17:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (joined_parcels\n",
    "#     >>star_map(to_parquet)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efeced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('base': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "14ab39836ad45f9872a4fbaf347177c599900dde8b629b89551b87d8978983f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
